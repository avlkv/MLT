{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple\n",
    "from scipy import stats\n",
    "from IPython.display import Image\n",
    "from sklearn.datasets import load_iris, load_boston\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, median_absolute_error, r2_score \n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC, OneClassSVM, SVR, NuSVR, LinearSVR\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "sns.set(style=\"ticks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray) -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Вычисление метрики accuracy для каждого класса\n",
    "    y_true - истинные значения классов\n",
    "    y_pred - предсказанные значения классов\n",
    "    Возвращает словарь: ключ - метка класса, \n",
    "    значение - Accuracy для данного класса\n",
    "    \"\"\"\n",
    "    # Для удобства фильтрации сформируем Pandas DataFrame \n",
    "    d = {'t': y_true, 'p': y_pred}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    # Метки классов\n",
    "    classes = np.unique(y_true)\n",
    "    # Результирующий словарь\n",
    "    res = dict()\n",
    "    # Перебор меток классов\n",
    "    for c in classes:\n",
    "        # отфильтруем данные, которые соответствуют \n",
    "        # текущей метке класса в истинных значениях\n",
    "        temp_data_flt = df[df['t']==c]\n",
    "        # расчет accuracy для заданной метки класса\n",
    "        temp_acc = accuracy_score(\n",
    "            temp_data_flt['t'].values, \n",
    "            temp_data_flt['p'].values)\n",
    "        # сохранение результата в словарь\n",
    "        res[c] = temp_acc\n",
    "    return res\n",
    "\n",
    "def print_accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    Вывод метрики accuracy для каждого класса\n",
    "    \"\"\"\n",
    "    accs = accuracy_score_for_classes(y_true, y_pred)\n",
    "    if len(accs)>0:\n",
    "        print('Метка \\t Accuracy')\n",
    "    for i in accs:\n",
    "        print('{} \\t {}'.format(i, accs[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка набора данных\n",
    "Для лабораторной работы будем использовать набор данных со [спам-сообщениями](https://www.kaggle.com/team-ai/spam-text-message-classification).\n",
    "<a id='data_desc'></a> \n",
    "\n",
    "<b>Задача </b> состоит в предсказании, является ли это сообщение спамом.\n",
    "\n",
    "Колонки:\n",
    "\n",
    "1. `Category` - спам/не-спам\n",
    "2. `Message` -  сообщение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5567</td>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5568</td>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5569</td>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5570</td>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5571</td>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category                                            Message\n",
       "0         ham  Go until jurong point, crazy.. Available only ...\n",
       "1         ham                      Ok lar... Joking wif u oni...\n",
       "2        spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3         ham  U dun say so early hor... U c already then say...\n",
       "4         ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...       ...                                                ...\n",
       "5567     spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568      ham               Will ü b going to esplanade fr home?\n",
       "5569      ham  Pity, * was in mood for that. So...any other s...\n",
       "5570      ham  The guy did some bitching but I acted like i'd...\n",
       "5571      ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/spam.csv', sep = ',')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category    object\n",
       "Message     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Типы данных в колонках\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка на наличие пропущенных значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category    0\n",
       "Message     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пропущенные значения не найдены."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем категориальный признак в числовой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "#пол\n",
    "le.fit(data.Category.drop_duplicates()) \n",
    "data.Category = le.transform(data.Category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "спам = 1\n",
    "\n",
    "не-спам = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Category                                            Message\n",
       "0         0  Go until jurong point, crazy.. Available only ...\n",
       "1         0                      Ok lar... Joking wif u oni...\n",
       "2         1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3         0  U dun say so early hor... U c already then say...\n",
       "4         0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Векторизация\n",
    "\n",
    "Сформируем общий словарь для обучения моделей из обучающей и тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ok lar... Joking wif u oni...',\n",
       " \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n",
       " 'U dun say so early hor... U c already then say...',\n",
       " \"Nah I don't think he goes to usf, he lives around here though\",\n",
       " \"FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\",\n",
       " 'Even my brother is not like to speak with me. They treat me like aids patent.',\n",
       " \"As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\",\n",
       " 'WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.',\n",
       " 'Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list = data['Message'].tolist()\n",
    "vocab_list[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создание векторизатора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество сформированных признаков - 8709\n"
     ]
    }
   ],
   "source": [
    "vocabVect = CountVectorizer()\n",
    "vocabVect.fit(vocab_list)\n",
    "corpusVocab = vocabVect.vocabulary_\n",
    "print('Количество сформированных признаков - {}'.format(len(corpusVocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "until=8080\n",
      "jurong=4370\n",
      "point=5954\n",
      "crazy=2334\n",
      "available=1313\n",
      "only=5567\n",
      "in=4110\n",
      "bugis=1763\n",
      "great=3651\n"
     ]
    }
   ],
   "source": [
    "for i in list(corpusVocab)[1:10]:\n",
    "    print('{}={}'.format(i, corpusVocab[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала в словаре идут все номера, которые попадались в спам-сообщениях:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0207',\n",
       " '02072069400',\n",
       " '02073162414',\n",
       " '02085076972',\n",
       " '021',\n",
       " '03',\n",
       " '04',\n",
       " '0430',\n",
       " '05',\n",
       " '050703']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabVect.get_feature_names()[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После них уже идут все обнаруженные слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['euro',\n",
       " 'euro2004',\n",
       " 'eurodisinc',\n",
       " 'europe',\n",
       " 'evaluation',\n",
       " 'evaporated',\n",
       " 'eve',\n",
       " 'eveb',\n",
       " 'even',\n",
       " 'evening']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabVect.get_feature_names()[3000:3010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Использование N-грам\n",
    "\n",
    "Создадим словарь, содержащий словосочетания, состоящие из 1, 2, 3 слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обнаружено словосочетаний: 104934\n"
     ]
    }
   ],
   "source": [
    "ncv = CountVectorizer(ngram_range=(1,3))\n",
    "ngram_features = ncv.fit_transform(vocab_list)\n",
    "print('Обнаружено словосочетаний:',len(ncv.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примеры словосочетаний:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10k cash', '10k cash or', '10p', '10p min', '10p min mob', '10p min only', '10p min stop', '10p min to', '10p per', '10p per min'] \n",
      "\n",
      "['and more when', 'and must', 'and must say', 'and my', 'and my amigos', 'and my constant', 'and my diet', 'and my man', 'and my mumhas', 'and my sister']\n"
     ]
    }
   ],
   "source": [
    "print(ncv.get_feature_names()[1000:1010],'\\n')\n",
    "print(ncv.get_feature_names()[7000:7010])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Решение задачи классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VectorizeAndClassify(vectorizers_list, classifiers_list):\n",
    "    for v in vectorizers_list:\n",
    "        for c in classifiers_list:\n",
    "            pipeline1 = Pipeline([(\"vectorizer\", v), (\"classifier\", c)])\n",
    "            score = cross_val_score(pipeline1, data['Message'], data['Category'], scoring='accuracy', cv=3).mean()\n",
    "            print('Векторизация - {}'.format(v))\n",
    "            print('Модель для классификации - {}'.format(c))\n",
    "            print('Accuracy = {}'.format(score))\n",
    "            print('===========================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\volko\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Векторизация - CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None,\n",
      "                vocabulary={'00': 0, '000': 1, '000pes': 2, '008704050406': 3,\n",
      "                            '0089': 4, '0121': 5, '01223585236': 6,\n",
      "                            '01223585334': 7, '0125698789': 8, '02': 9,\n",
      "                            '0207': 10, '02072069400': 11, '02073162414': 12,\n",
      "                            '02085076972': 13, '021': 14, '03': 15, '04': 16,\n",
      "                            '0430': 17, '05': 18, '050703': 19, '0578': 20,\n",
      "                            '06': 21, '07': 22, '07008009200': 23,\n",
      "                            '07046744435': 24, '07090201529': 25,\n",
      "                            '07090298926': 26, '07099833605': 27,\n",
      "                            '07123456789': 28, '0721072': 29, ...})\n",
      "Модель для классификации - LogisticRegression(C=3.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Accuracy = 0.9825916889690364\n",
      "===========================\n",
      "Векторизация - CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None,\n",
      "                vocabulary={'00': 0, '000': 1, '000pes': 2, '008704050406': 3,\n",
      "                            '0089': 4, '0121': 5, '01223585236': 6,\n",
      "                            '01223585334': 7, '0125698789': 8, '02': 9,\n",
      "                            '0207': 10, '02072069400': 11, '02073162414': 12,\n",
      "                            '02085076972': 13, '021': 14, '03': 15, '04': 16,\n",
      "                            '0430': 17, '05': 18, '050703': 19, '0578': 20,\n",
      "                            '06': 21, '07': 22, '07008009200': 23,\n",
      "                            '07046744435': 24, '07090201529': 25,\n",
      "                            '07090298926': 26, '07099833605': 27,\n",
      "                            '07123456789': 28, '0721072': 29, ...})\n",
      "Модель для классификации - LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "          verbose=0)\n",
      "Accuracy = 0.9834887108563705\n",
      "===========================\n",
      "Векторизация - CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None,\n",
      "                vocabulary={'00': 0, '000': 1, '000pes': 2, '008704050406': 3,\n",
      "                            '0089': 4, '0121': 5, '01223585236': 6,\n",
      "                            '01223585334': 7, '0125698789': 8, '02': 9,\n",
      "                            '0207': 10, '02072069400': 11, '02073162414': 12,\n",
      "                            '02085076972': 13, '021': 14, '03': 15, '04': 16,\n",
      "                            '0430': 17, '05': 18, '050703': 19, '0578': 20,\n",
      "                            '06': 21, '07': 22, '07008009200': 23,\n",
      "                            '07046744435': 24, '07090201529': 25,\n",
      "                            '07090298926': 26, '07099833605': 27,\n",
      "                            '07123456789': 28, '0721072': 29, ...})\n",
      "Модель для классификации - KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "Accuracy = 0.9122387985297536\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\volko\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Векторизация - TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use...\n",
      "                vocabulary={'00': 0, '000': 1, '000pes': 2, '008704050406': 3,\n",
      "                            '0089': 4, '0121': 5, '01223585236': 6,\n",
      "                            '01223585334': 7, '0125698789': 8, '02': 9,\n",
      "                            '0207': 10, '02072069400': 11, '02073162414': 12,\n",
      "                            '02085076972': 13, '021': 14, '03': 15, '04': 16,\n",
      "                            '0430': 17, '05': 18, '050703': 19, '0578': 20,\n",
      "                            '06': 21, '07': 22, '07008009200': 23,\n",
      "                            '07046744435': 24, '07090201529': 25,\n",
      "                            '07090298926': 26, '07099833605': 27,\n",
      "                            '07123456789': 28, '0721072': 29, ...})\n",
      "Модель для классификации - LogisticRegression(C=3.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Accuracy = 0.9712851555775054\n",
      "===========================\n",
      "Векторизация - TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use...\n",
      "                vocabulary={'00': 0, '000': 1, '000pes': 2, '008704050406': 3,\n",
      "                            '0089': 4, '0121': 5, '01223585236': 6,\n",
      "                            '01223585334': 7, '0125698789': 8, '02': 9,\n",
      "                            '0207': 10, '02072069400': 11, '02073162414': 12,\n",
      "                            '02085076972': 13, '021': 14, '03': 15, '04': 16,\n",
      "                            '0430': 17, '05': 18, '050703': 19, '0578': 20,\n",
      "                            '06': 21, '07': 22, '07008009200': 23,\n",
      "                            '07046744435': 24, '07090201529': 25,\n",
      "                            '07090298926': 26, '07099833605': 27,\n",
      "                            '07123456789': 28, '0721072': 29, ...})\n",
      "Модель для классификации - LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "          verbose=0)\n",
      "Accuracy = 0.9798994639895708\n",
      "===========================\n",
      "Векторизация - TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use...\n",
      "                vocabulary={'00': 0, '000': 1, '000pes': 2, '008704050406': 3,\n",
      "                            '0089': 4, '0121': 5, '01223585236': 6,\n",
      "                            '01223585334': 7, '0125698789': 8, '02': 9,\n",
      "                            '0207': 10, '02072069400': 11, '02073162414': 12,\n",
      "                            '02085076972': 13, '021': 14, '03': 15, '04': 16,\n",
      "                            '0430': 17, '05': 18, '050703': 19, '0578': 20,\n",
      "                            '06': 21, '07': 22, '07008009200': 23,\n",
      "                            '07046744435': 24, '07090201529': 25,\n",
      "                            '07090298926': 26, '07099833605': 27,\n",
      "                            '07123456789': 28, '0721072': 29, ...})\n",
      "Модель для классификации - KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "Accuracy = 0.924806089662772\n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "vectorizers_list = [CountVectorizer(vocabulary = corpusVocab), TfidfVectorizer(vocabulary = corpusVocab)]\n",
    "classifiers_list = [LogisticRegression(C=3.0), LinearSVC(), KNeighborsClassifier()]\n",
    "VectorizeAndClassify(vectorizers_list, classifiers_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все модели имеют высокие показатели качества, даже метод ближайших соседей в CountVectorizer имеет точность 0.91.\n",
    "\n",
    "Лучшая точно была получения при использовании CountVectorizer с методом опорных векторов. Точность: 0.983.\n",
    "\n",
    "Интересно, что с логистической регрессией и методом опорных веткоров хуже себя показал TfidfVectorizer, хотя ожидалось обратное."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разделим выборку на обучающую и тестовую и проверим решение для лучшей модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(v, c):\n",
    "    model = Pipeline(\n",
    "        [(\"vectorizer\", v), \n",
    "         (\"classifier\", c)])\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_pred = model.predict(X_test)\n",
    "    print_accuracy_score_for_classes(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(data['Message'],\n",
    "                                                    data['Category'],\n",
    "                                                    test_size=0.05,\n",
    "                                                    random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 1.0\n",
      "1 \t 0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "sentiment(CountVectorizer(), LinearSVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 1.0\n",
      "1 \t 0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "sentiment(CountVectorizer(ngram_range=(1,3)), LinearSVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 1.0\n",
      "1 \t 0.75\n"
     ]
    }
   ],
   "source": [
    "sentiment(CountVectorizer(ngram_range=(2,3)), LinearSVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 1.0\n",
      "1 \t 0.8611111111111112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\volko\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "sentiment(CountVectorizer(ngram_range=(1,4)), LinearSVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 1.0\n",
      "1 \t 0.7222222222222222\n"
     ]
    }
   ],
   "source": [
    "sentiment(CountVectorizer(ngram_range=(2,4)), LinearSVC())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучше всего себя показала комбинация просто векторизатора без словосочетаний. Также видно, что при удалении из N-грам словосочетаний длиной 1 точность падает.\n",
    "\n",
    "Скорее всего дело в том, что спам-сообщения легко определить по ключевым словам, таким как 'free', 'coupon' и т.д., а также по странным комбинациям цифр, представляющим собой какой-то промо-код или номер телефона.\n",
    "\n",
    "Словосочетания уже роли особой не играют."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Решение задачи классификации текстов с использованием Байесовских методов\n",
    "\n",
    "Оценим распределение целевого признака"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data['Category'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Спама намного меньше.\n",
    "\n",
    "Для выборок с сильным дисбалансом классов принято применять <b>Complement Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 0.9917695473251029\n",
      "1 \t 0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "sentiment(CountVectorizer(), ComplementNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 0.9958847736625515\n",
      "1 \t 0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "sentiment(TfidfVectorizer(), ComplementNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точность для метки 0 упала совсем незначительно по сравнению с CountVectorizer с методом опорных векторов, зато сильно выросла точность для метки 1.\n",
    "\n",
    "Задачей для данного набора данных является точное определение спам-сообщений, поэтому точность определения метки 1 намного важнее.\n",
    "\n",
    "Наивный Байесовский метод показал себя лучше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем другие методы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 0.9958847736625515\n",
      "1 \t 0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "sentiment(CountVectorizer(), MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 1.0\n",
      "1 \t 0.7222222222222222\n"
     ]
    }
   ],
   "source": [
    "sentiment(TfidfVectorizer(), MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 0.9958847736625515\n",
      "1 \t 0.8611111111111112\n"
     ]
    }
   ],
   "source": [
    "sentiment(CountVectorizer(binary=True), BernoulliNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшим можно считать pipeline из CountVectorizer и  Complement Naive Bayes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
